


# Number of parallel network streams. proton block runs are already parallel in nature.
# So please do not increase the additional parallelism within the uploader for proton 
# runs.  A value of 5 is optimum for pgm runs, and a value of 1 is optimum for proton runs 
# A value of 5 is optimum for iru-cli and iru-app runs. 
# IRU app uses same keys as that of IRU-cli. The values for cli can be overridden
# for different kinds of OS. 32bit Windows does show some issues for acquiring large 
# contiguous memory by jvm. Hence cli running on windows should use lower number. A value of 
# 3 is optimum for cli running on windows.
cli.numParallelStreams=5
cli.win.numParallelStreams=3
cli.linux.numParallelStreams=5
cli.mac.numParallelStreams=5
pgm.numParallelStreams=5
proton.numParallelStreams=1


# segment size, for each file segment in a parallel stream. optimum value of 128MB 
# for pgm runs, 16MB for proton runs and 64MB for iru-cli or iru-app runs. 
# IRU app uses same keys as that of cli. The values for cli can be overridden
# for different kinds of OS. 32bit Windows does show some issues for acquiring large 
# contiguous memory by jvm. Hence cli running on windows should use lower number. A value of 
# 32MB  is optimum for cli running on windows.
cli.fileSegmentSize=64MB
cli.win.fileSegmentSize=32MB
cli.linux.fileSegmentSize=64MB
cli.mac.fileSegmentSize=64MB
pgm.fileSegmentSize=128MB
proton.fileSegmentSize=16MB


#overrides any of the above number of parallel streams and file segment sizes, if the
# IR server is a local IR server(not cloud based). Owing to more number of TS
#connecting to same IR, and IR doesnt have enough memory compared to cloud.
local.ir.numParallelStreams=5
local.ir.fileSegmentSize=64MB



# retry counts. A file is broken down into segments, if file size is greater than a 
# certain threshold size.  Each segment upload will be retried 12 times by default.
# Each file, as such will be tried 8 times by default, besides individual segment
# retries of each file. 
# Data Transfer of files is associated with a number of control signal ws-api calls
# that go before, during and after the data transfer. Those control signals will 
# also be retried 8 times by default.
# Use the following parameters to over ride the retry counts. 
file.upload.retry.count=8
segment.upload.retry.count=12
txfr.ctrl.api.retry.count=8

# time out value (in seconds) for webservice calls made from IRU to the IR
#Not implemented yet.
IRWebServiceCallTimeOut=90

# bam files containing multiple read groups, are detected and not allowed to be
# uploaded by the iru-cli and iru-app. IR analysis fails on such .bam files.
# Valid values are true or false (no quotes). Default is false.
# If user wants to upload such files for future use, then user can turn on 
# this option. Highly recommended that this be kept as false.
cli.multiple.RG.bam.uploads.allowed=false

# use this option only if the direct internet connection has trouble 
# and s3 uploads are failing frequently.
slower.two.stage.S3.upload=false

###################################################
###################################################
##  
##     AmazonAWS S3 storage access parameters
##  
# max http connection
# maximum number of S3 open http request pool size. default is 50 from amazon.
amazonaws.s3.max.http.connection = 50
# connection timeout 
# timeout for creating new S3 connections, in milliseconds. default is 
# 50000 (50 seconds) from amazon.
# 1800000 = 1800 seconds = 30 minutes
amazonaws.s3.connection.timeout = 1800000
# socket timeout 
# timeout for reading from an S3 conected socket, in milliseconds. default is 
# 50000 (50 seconds) from amazon.
# 1800000 = 1800 seconds = 30 minutes
amazonaws.s3.socket.timeout = 1800000
# connection ttl
# timeout for reading from an S3 conected socket, in milliseconds. default is 
# -1 from amazon.
#amazonaws.s3.connection.ttl = -1
# tcp keepalive
# whether to keep alive tcp connection on S3 conected socket, true / false, default is 
# false from amazon.
amazonaws.s3.tcp.keepalive = false
# max error retry
# The maximum number of times, to retry a failed upload. default is ___ from amazon.
amazonaws.s3.max.error.retry = 5












