#!/usr/bin/env python
# Copyright (C) 2013 Ion Torrent Systems, Inc. All Rights Reserved

import sys
import os
import re
import subprocess
import json
import shutil
import time
import traceback
import unicodedata
import copy
from optparse import OptionParser
from django.conf import settings
from django.template.loader import render_to_string
from ion.utils import compress # provided by ion-pipeline

# critical environment variables:
DIRNAME                     = '' # home directory for the plugin files
TSP_URLPATH_PLUGIN_DIR      = ''
TSP_FILEPATH_PLUGIN_DIR     = ''
startplugin_json            = {}

# File names generated by the plugin
BASENAME_VARIANTS_XLS       = 'variants.xls'
BASENAME_ALLELES_XLS        = 'alleles.xls'
BASENAME_HOTSPOTS_XLS       = 'allele_counts.xls'
BASENAME_VARIANTS_VCF       = 'TSVC_variants.vcf'
BASENAME_GENOME_VCF         = 'TSVC_variants.genome.vcf'
BASENAME_PARAMETERS_JSON    = 'local_parameters.json'
HTML_BLOCK                  = 'variantCaller_block.html'    # Top report page block
HTML_RESULTS                = 'variantCaller.html'          # Main plugin page
BASENAME_VARIANT_COV_XLS    = 'variant_allele_counts.xls'


# DEVELOPMENT/DEBUG options:
# NOTE: the following should all be set to 0 in production mode
PLUGIN_DEV_KEEP_INTERMEDIATE_FILES = True   # use prior to PLUGIN_DEV_RECALL_VARIANTS=1 to re-process from temporary results
PLUGIN_DEV_SKIP_VARIANT_CALLING = False      # 1 to skip variant calling - use previous calls
SKIP_BAMFILE_VERSION_CHECK = False

# Minimum barcode BAM size required for variant calling. 50,000 bytes ~ 100-400 reads.
BCFILE_MIN_SIZE = 50000

output_files = []


def printtime(message, *args):
    if args:
        message = message % args
    print "[ " + time.strftime('%X') + " ] " + message
    sys.stdout.flush()
    sys.stderr.flush()


def unicode_cleanup(message):
    try:
        return str(message)
    except:
        return unicodedata.normalize('NFKD',unicode(message)).encode('ascii','ignore')

def run_command(command,description):
    printtime(' ')
    printtime('Task    : ' + description)
    printtime('Command : ' + command)
    printtime(' ')
    return subprocess.call(command,shell=True)

def execute_output(cmd):
    try:
        process = subprocess.Popen(cmd, stderr=subprocess.STDOUT, stdout=subprocess.PIPE, shell=True)
        return process.communicate()[0]
    except:
        traceback.print_exc()
        return ''


def generate_incomplete_report_page(output_html_filename, message, vc_options, autorefresh=False):
    run_name = ''
    if ('run_name' in vc_options): run_name = vc_options['run_name']
    render_context = { 'run_name' : run_name, 'message' : message, 'autorefresh' : autorefresh,
                       'startplugin_json' : startplugin_json}

    out = open(output_html_filename,'w')
    out.write(render_to_string('report_incomplete.html', render_context))
    out.close()


def generate_nonbarcoded_block_page(output_html_filename, message):
    out = open(output_html_filename,'w')
    cssStr = """<link rel="stylesheet" media="all" href="/site_media/resources/bootstrap/css/bootstrap.min.css">"""
    out.write("<!DOCTYPE html><html>" + cssStr + "<body><p>" + message + "</p></body></html>\n")
    out.close()


def generate_barcode_links_block(block_html_path, barcode_data, vc_options):

    render_context = {
        'barcode_data'          : barcode_data,
        'options'               : vc_options,
        'startplugin_json'      : startplugin_json
    }

    out = open(block_html_path,'w')
    out.write(render_to_string('block_barcodes.html', render_context))
    out.close()



def generate_barcode_links_page (results_html_path, barcode_data, vc_options):

    render_context = {
        'barcode_data'     : barcode_data,
        'options'          : vc_options,
        'autorefresh'      : False,
        'startplugin_json' : startplugin_json
    }

    for barcode in barcode_data:
        if barcode['status'] == 'in_progress':
            render_context['autorefresh'] = True

    out = open(results_html_path,'w')
    out.write(render_to_string('report_barcodes.html', render_context))
    out.close()



def add_output_file(type, filename, barcode=None, sample=None):

    global output_files
    global TSP_URLPATH_PLUGIN_DIR
    global TSP_FILEPATH_PLUGIN_DIR

    file_info = {
        'type' : type,
        'filename' : os.path.basename(filename),
        'server_path' : os.path.join(TSP_FILEPATH_PLUGIN_DIR,filename),
        'download_path' : os.path.join(TSP_URLPATH_PLUGIN_DIR,filename)
    }
    if barcode is not None:
        file_info['barcode'] = barcode
    if sample is not None:
        file_info['sample'] = sample

    output_files.append(file_info)


def call_variants(dataset):

    printtime("dataset: %s" % dataset) #TODO DEBUG

    if (not 'name' in dataset): dataset['name'] = 'default'
    if dataset['is_barcode']:
        barcode_modifier       = '../'
        basename_variants_vcf  = 'TSVC_variants_%s.vcf' % dataset['name']
        basename_genome_vcf    = 'TSVC_variants_%s.genome.vcf' % dataset['name']
        basename_variants_xls  = 'variants_%s.xls' % dataset['name']
        basename_hotspots_xls  = 'allele_counts_%s.xls' % dataset['name']
        basename_alleles_xls   = 'alleles_%s.xls' % dataset['name']
        results_directory      = TSP_FILEPATH_PLUGIN_DIR + '/' + dataset['name']
        basename_variant_cov_xls  = 'variant_allele_counts_%s.xls' % dataset['name']
    else:
        barcode_modifier = ''
        basename_variants_vcf  = BASENAME_VARIANTS_VCF
        basename_genome_vcf    = BASENAME_GENOME_VCF
        basename_variants_xls  = BASENAME_VARIANTS_XLS
        basename_hotspots_xls  = BASENAME_HOTSPOTS_XLS
        basename_alleles_xls   = BASENAME_ALLELES_XLS
        results_directory      = TSP_FILEPATH_PLUGIN_DIR
        basename_variant_cov_xls  = BASENAME_VARIANT_COV_XLS

    if not os.path.exists(results_directory):
        os.makedirs(results_directory)

    basename_input_bam   = os.path.basename(dataset['bam'])
    untrimmed_bam = os.path.join(results_directory,basename_input_bam)                      # Local symlink to untrimmed BAM
    processed_bam = os.path.join(results_directory,basename_input_bam[:-4] + '_processed.bam')

    if dataset['bam'] != untrimmed_bam:
        os.symlink(dataset['bam'],        untrimmed_bam)
        os.symlink(dataset['bam']+'.bai', untrimmed_bam+'.bai')

    assert not (dataset['trim_reads'] and not dataset['has_targets']), "Read trimming enabled but targets BED not provided"

    # Execute main variant caller script
    variantcaller_command        = '%s/bin/variant_caller_pipeline.py' % DIRNAME
    variantcaller_command   +=     '  --input-bam "%s"' % untrimmed_bam
    if dataset['trim_reads']:
        variantcaller_command   += '  --primer-trim-bed "%s"' % dataset['targets_bed_unmerged']
        variantcaller_command   += '  --postprocessed-bam "%s"' % processed_bam
    variantcaller_command       += '  --reference-fasta "%s"' % dataset['reference_genome_fasta']
    variantcaller_command       += '  --output-dir "%s"' % results_directory
    variantcaller_command       += '  --parameters-file "%s"' % os.path.join(TSP_FILEPATH_PLUGIN_DIR,BASENAME_PARAMETERS_JSON) # TODO: barcode specific, or better application type specific ?
    variantcaller_command       += '  --bin-dir "%s/bin"' % DIRNAME
    if dataset['has_targets']:
        variantcaller_command   += '  --region-bed "%s"' % dataset['targets_bed_merged']
    if dataset['has_hotspots']:
        variantcaller_command   += '  --hotspot-vcf "%s"' % dataset['hotspots_vcf']
    if dataset['has_error_motifs']:
        variantcaller_command   += '  --error-motifs "%s"' % dataset['error_motifs']
    variantcaller_command       += '  --generate-gvcf on'

    if PLUGIN_DEV_SKIP_VARIANT_CALLING:
        printtime('Skipping calling variants on mapped reads...')
    else:
        run_command(variantcaller_command,'Execute variant caller script')


    # Generate allele counts if hotspots loci BED provided
    if dataset['has_hotspots']:
        allelecount_command = 'samtools mpileup -BQ0 -d1000000'
        allelecount_command += ' -f "%s"' % dataset['reference_genome_fasta']
        allelecount_command += ' -l ' + dataset['hotspots_bed_merged']
        if dataset['trim_reads']:
            allelecount_command += ' ' + processed_bam
        else:
            allelecount_command += ' ' + untrimmed_bam
        allelecount_command += ' | %s/scripts/allele_count_mpileup_stdin.py' % DIRNAME
        allelecount_command += ' > ' + os.path.join(results_directory,'allele_counts.txt')
        if PLUGIN_DEV_SKIP_VARIANT_CALLING:
            printtime('Skipping base pileup for hotspot alleles...')
        else:
            run_command(allelecount_command,'Base pileup for hotspot alleles')

        allelecount2_command = '%s/scripts/print_allele_counts.py' % DIRNAME
        allelecount2_command += ' ' + os.path.join(results_directory,'allele_counts.txt')
        allelecount2_command += ' ' + os.path.join(results_directory,BASENAME_HOTSPOTS_XLS)
        allelecount2_command += ' "%s"' % dataset['hotspots_bed_unmerged_leftalign']
        allelecount2_command += ' "%s"' % dataset['hotspots_bed_unmerged']
        run_command(allelecount2_command,'Generate hotspots allele coverage')

    # Generate xls tables and statistics from final vcf

    tvc_args = startplugin_json['pluginconfig']['meta'].get('tvcargs','')
    table_command        = '%s/scripts/generate_variant_tables.py' % DIRNAME
    if (tvc_args.find("--suppress-no-calls off") != -1): 
        table_command   += '  --suppress-no-calls off'
    else: 
        table_command   += '  --suppress-no-calls on'
    table_command       += '  --input-vcf %s'       % os.path.join(results_directory,BASENAME_VARIANTS_VCF)
    if dataset['has_targets']:
        table_command   += '  --region-bed "%s"'    % dataset['targets_bed_unmerged']
    if dataset['has_hotspots']:
        table_command   += '  --hotspots'
    table_command       += '  --output-xls %s'      % os.path.join(results_directory,BASENAME_VARIANTS_XLS)
    table_command       += '  --alleles2-xls %s'    % os.path.join(results_directory,BASENAME_ALLELES_XLS)
    table_command       += '  --summary-json %s'    % os.path.join(results_directory,'variant_summary.json')
    table_command       += '  --scatter-png %s'     % os.path.join(results_directory,'scatter.png')
    if dataset['is_barcode']:
        table_command   += '  --barcode %s'  % dataset['name']
        table_command   += '  --concatenated-xls "%s/%s.xls"' % (TSP_FILEPATH_PLUGIN_DIR,dataset['run_name'])
    table_command       += '  --run-name "%s"'  % dataset['run_name']


    run_command(table_command,'Generate xls tables and statistics from final vcf')


    # Transfer variants to sqllite database
    subprocess.call('rm -f %s'     % os.path.join(results_directory,'alleles.db'), shell=True)

    sqllite_command        = 'python %s/scripts/csv2sqlite.py' % DIRNAME
    sqllite_command       += '  %s'     % os.path.join(results_directory,BASENAME_ALLELES_XLS)
    sqllite_command       += '  %s'     % os.path.join(results_directory,'alleles.db')
    sqllite_command       += '  variants'
    run_command(sqllite_command,'Transfer variants to sqllite database')


    # Create symlinks to js/css folders and php scripts # static data
    subprocess.call('ln -sf "%s/slickgrid" "%s"' % (DIRNAME,results_directory),shell=True)
    subprocess.call('cp -rf %s/copytoreport/* "%s"' % (DIRNAME,results_directory),shell=True)
    subprocess.call('ln -sf %s/scripts/*.php3 "%s"' % (DIRNAME,results_directory),shell=True)

    if dataset['is_barcode']:
        os.symlink(os.path.join(results_directory,BASENAME_VARIANTS_VCF+'.gz'),
                   os.path.join(results_directory,basename_variants_vcf+'.gz'))
        os.symlink(os.path.join(results_directory,BASENAME_VARIANTS_VCF+'.gz.tbi'),
                   os.path.join(results_directory,basename_variants_vcf+'.gz.tbi'))
        os.symlink(os.path.join(results_directory,BASENAME_GENOME_VCF+'.gz'),
                   os.path.join(results_directory,basename_genome_vcf+'.gz'))
        os.symlink(os.path.join(results_directory,BASENAME_GENOME_VCF+'.gz.tbi'),
                   os.path.join(results_directory,basename_genome_vcf+'.gz.tbi'))
        os.symlink(os.path.join(results_directory,BASENAME_VARIANTS_XLS),
                   os.path.join(results_directory,basename_variants_xls))
        os.symlink(os.path.join(results_directory,BASENAME_ALLELES_XLS),
                   os.path.join(results_directory,basename_alleles_xls))
        if dataset['has_hotspots']:
            os.symlink(os.path.join(results_directory,BASENAME_HOTSPOTS_XLS),
                       os.path.join(results_directory,basename_hotspots_xls))

    subprocess.call('touch %s/%s.done' % (results_directory,basename_variants_vcf),shell=True)

    render_context = {
        'options'               : dataset,
        'configuration_link'    : BASENAME_PARAMETERS_JSON,
        'mapped_bam_link'       : os.path.basename(untrimmed_bam),
        'mapped_bai_link'       : os.path.basename(untrimmed_bam)+'.bai',
        'variants_vcf_gz_link'  : basename_variants_vcf+'.gz',
        'variants_tbi_link'     : basename_variants_vcf+'.gz.tbi',
        'genome_vcf_gz_link'    : basename_genome_vcf+'.gz',
        'genome_tbi_link'       : basename_genome_vcf+'.gz.tbi',
        'variants_xls_link'     : basename_variants_xls,
        'alleles_xls_link'      : basename_alleles_xls,
        'hotspots_xls_link'     : basename_hotspots_xls,
        'variant_cov_xls_link'  : basename_variant_cov_xls,
        'processed_bam_link'    : os.path.basename(processed_bam),
        'processed_bai_link'    : os.path.basename(processed_bam)+'.bai',
        'results_url'           : TSP_URLPATH_PLUGIN_DIR,
        'startplugin_json'      : startplugin_json
    }

    if dataset['is_barcode']:
        render_context['results_url'] += '/' + dataset['name']

    summary_in = open(os.path.join(results_directory,'variant_summary.json'))
    render_context['summary'] = json.load(summary_in)
    summary_in.close()

    tvcutils_command = "tvcutils prepare_hotspots"
    tvcutils_command += ' --reference "%s"' % dataset['reference_genome_fasta']
    tvcutils_command += ' --input-vcf "%s"' % os.path.join(results_directory,BASENAME_VARIANTS_VCF)
    tvcutils_command += ' --output-bed "%s"' % os.path.join(results_directory,'TSVC_variants.bed')
    run_command(tvcutils_command,'Write variants bed')
    allelecount_command = 'samtools mpileup -BQ0 -d1000000'
    allelecount_command += ' -f "%s"' % dataset['reference_genome_fasta']
    allelecount_command += ' -l ' + os.path.join(results_directory,'TSVC_variants.bed')
    if dataset['trim_reads']:
        allelecount_command += ' ' + processed_bam
    else:
        allelecount_command += ' ' + untrimmed_bam
    allelecount_command += ' | %s/scripts/allele_count_mpileup_stdin.py' % DIRNAME
    allelecount_command += ' > ' + os.path.join(results_directory,'TSVC_variants_allele_counts.txt')
    if PLUGIN_DEV_SKIP_VARIANT_CALLING:
        printtime('Skipping base pileup for variant alleles...')
    else:
        run_command(allelecount_command,'Base pileup for variant alleles')
    allelecount2_command = '%s/scripts/print_variant_allele_counts.py' % DIRNAME
    allelecount2_command += ' ' + dataset['name']
    allelecount2_command += " '" + render_context['summary']['sample_name'] + "'"
    allelecount2_command += ' ' + os.path.join(results_directory,BASENAME_VARIANTS_VCF)
    allelecount2_command += ' ' + os.path.join(results_directory,'TSVC_variants_allele_counts.txt')
    allelecount2_command += ' ' + os.path.join(results_directory,BASENAME_VARIANT_COV_XLS)
    run_command(allelecount2_command,'Generate variant allele coverage')

    if dataset['is_barcode']:
        os.symlink(os.path.join(results_directory,BASENAME_VARIANT_COV_XLS),
                   os.path.join(results_directory,basename_variant_cov_xls))

    if dataset['has_targets']:
        render_context['targets_bed_link'] = os.path.basename(dataset['targets_bed_unmerged'])

    if dataset['has_hotspots']:
        render_context['hotspots_bed_link'] = os.path.basename(dataset['hotspots_bed_unmerged_local'])

    if dataset['has_targets'] and dataset['trim_reads']:
        render_context['effective_regions_bed_link'] = 'effective_regions.bed'

    if dataset['is_barcode']:
        render_context['barcode'] = dataset['name']

    out = open(results_directory + '/' + HTML_RESULTS,'w')
    out.write(render_to_string('report_details.html', render_context))
    out.close()

    out = open(results_directory + '/' + HTML_BLOCK,'w')
    out.write(render_to_string('block_details.html', render_context))
    out.close()

    out = open(results_directory + '/deprecated.htm','w')
    out.write(render_to_string('report_deprecated.html', render_context))
    out.close()


    # Create xml template required for adding IGV links
    fxml = open(os.path.join(results_directory,'igv_session.xml'), "w")
    fxml.write('<?xml version="1.0" encoding="UTF-8" standalone="no"?>\n')
    if (dataset['reference_genome_name'] == 'hg19'):
        fxml.write('<Global genome="%s" version="3">\n' % dataset['reference_genome_name'])
    else:    
        fxml.write('<Global genome="{plugin_url}/%s.fasta" version="3">\n' % (barcode_modifier + dataset['reference_genome_name']))
    fxml.write('    <Resources>\n')
    fxml.write('        <Resource name="%s.gz" path="{plugin_url}/%s.gz"/>\n' % (basename_variants_vcf,basename_variants_vcf))
    if dataset['trim_reads']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (os.path.basename(processed_bam),os.path.basename(processed_bam)))
    else:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (os.path.basename(untrimmed_bam),os.path.basename(untrimmed_bam)))
    if dataset['has_targets']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (render_context['targets_bed_link'],barcode_modifier+render_context['targets_bed_link']))
    if dataset['has_hotspots']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (render_context['hotspots_bed_link'],barcode_modifier+render_context['hotspots_bed_link']))
    if dataset['has_targets'] and dataset['trim_reads']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (render_context['effective_regions_bed_link'],render_context['effective_regions_bed_link']))
    fxml.write('    </Resources>\n')
    fxml.write('    <Panel name="DataPanel" height="150">\n')
    fxml.write('        <Track displayMode="EXPANDED" id="{plugin_url}/%s.gz" name="Variant Calls" visible="true"/>\n' % basename_variants_vcf)
    if dataset['has_targets']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s" name="%s" visible="true"/>\n' % (barcode_modifier+render_context['targets_bed_link'],dataset['targets_name']))
    if dataset['has_hotspots']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s" name="%s" visible="true"/>\n' % (barcode_modifier+render_context['hotspots_bed_link'],dataset['hotspots_name']))
    if dataset['has_targets'] and dataset['trim_reads']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s" name="%s" visible="true"/>\n' % (render_context['effective_regions_bed_link'],dataset['targets_name'] + '_effective'))
    fxml.write('    </Panel>\n')
    fxml.write('    <Panel height="525">\n')
    if dataset['trim_reads']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s_coverage" name="Coverage" visible="true"/>\n' % os.path.basename(processed_bam))
        fxml.write('        <Track displayMode="EXPANDED" id="{plugin_url}/%s" name="Alignments" visible="true"/>\n' % os.path.basename(processed_bam))
    else:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s_coverage" name="Coverage" visible="true"/>\n' % os.path.basename(untrimmed_bam))
        fxml.write('        <Track displayMode="EXPANDED" id="{plugin_url}/%s" name="Alignments" visible="true"/>\n' % os.path.basename(untrimmed_bam))
    fxml.write('    </Panel>\n')
    fxml.write('    <Panel name="FeaturePanel" height="75">\n')
    fxml.write('        <Track displayMode="COLLAPSED" id="Reference sequence" name="Reference sequence" visible="true"/>\n')
    fxml.write('    </Panel>\n')
    fxml.write('    <PanelLayout dividerFractions="0.20,0.75"/>\n')
    fxml.write('</Global>\n')
    fxml.close()



    # List of generated files:
    if dataset['is_barcode']:
        add_output_file('variants_vcf_gz', dataset['name']+'/'+basename_variants_vcf+'.gz', dataset['name'], render_context['summary']['sample_name'])
        add_output_file('variants_vcf_gz_tbi', dataset['name']+'/'+basename_variants_vcf+'.gz.tbi', dataset['name'], render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz', dataset['name']+'/'+basename_genome_vcf+'.gz', dataset['name'], render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz_tbi', dataset['name']+'/'+basename_genome_vcf+'.gz.tbi', dataset['name'], render_context['summary']['sample_name'])
        add_output_file('alleles_xls', dataset['name']+'/'+basename_alleles_xls, dataset['name'], render_context['summary']['sample_name'])
        add_output_file('mapped_bam', dataset['name']+'/'+os.path.basename(untrimmed_bam), dataset['name'], render_context['summary']['sample_name'])
        add_output_file('mapped_bam_bai', dataset['name']+'/'+os.path.basename(untrimmed_bam)+'.bai', dataset['name'], render_context['summary']['sample_name'])
        if dataset['trim_reads']:
            add_output_file('processed_bam', dataset['name']+'/'+os.path.basename(processed_bam), dataset['name'], render_context['summary']['sample_name'])
            add_output_file('processed_bam_bai', dataset['name']+'/'+os.path.basename(processed_bam)+'.bai', dataset['name'], render_context['summary']['sample_name'])
        add_output_file('filtered_variants_vcf', dataset['name']+'/small_variants_filtered.vcf', dataset['name'], render_context['summary']['sample_name'])

    else:
        add_output_file('variants_vcf_gz', basename_variants_vcf+'.gz', sample=render_context['summary']['sample_name'])
        add_output_file('variants_vcf_gz_tbi', basename_variants_vcf+'.gz.tbi', sample=render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz', basename_genome_vcf+'.gz', sample=render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz_tbi', basename_genome_vcf+'.gz.tbi', sample=render_context['summary']['sample_name'])
        add_output_file('alleles_xls', basename_alleles_xls, sample=render_context['summary']['sample_name'])
        add_output_file('mapped_bam', os.path.basename(untrimmed_bam), sample=render_context['summary']['sample_name'])
        add_output_file('mapped_bam_bai', os.path.basename(untrimmed_bam)+'.bai', sample=render_context['summary']['sample_name'])
        if dataset['trim_reads']:
            add_output_file('processed_bam', os.path.basename(processed_bam), sample=render_context['summary']['sample_name'])
            add_output_file('processed_bam_bai', os.path.basename(processed_bam)+'.bai', sample=render_context['summary']['sample_name'])
        add_output_file('filtered_variants_vcf', 'small_variants_filtered.vcf', sample=render_context['summary']['sample_name'])

    return render_context['summary']



def get_options(startplugin_json):
    ''' Attempt to get plugin options '''

    options = {}
    try:
        options['parameters']       = copy.deepcopy(startplugin_json['pluginconfig'])
        configuration               = options['parameters']['meta']['configuration']
    except:
        if ('barcodes' in startplugin_json['pluginconfig']):
            startplugin_json['pluginconfig']['meta'] = copy.deepcopy(startplugin_json['pluginconfig']['barcodes'][0]['json']['pluginconfig']['meta'])
        try:
            options['parameters']       = copy.deepcopy(startplugin_json['pluginconfig'])
            configuration               = options['parameters']['meta']['configuration']
        except:    
            # TODO: Autostart without configuration no longer allowed
            return {'error':'Automatic analysis was not performed. Plugin does not appear to be configured.'}

    # TODO, remove the folling lines, this needs to be handled one level higher
    # hard code for existing behavior with exception around HiQ on Proton
    options['has_error_motifs'] = True
    options['error_motifs'] = os.path.join(DIRNAME,'share/TVC/sse/motifset.txt')
    try:
        expmeta = startplugin_json['expmeta']
        plan = startplugin_json['plan']
        if plan['samplePrepKitName'] == 'Ion AmpliSeq Exome Kit' and expmeta['chiptype'] == 'P1.1.17' and plan['sequencekitname'] == 'IonProtonIHiQ':
            options['error_motifs'] =  os.path.join(DIRNAME,'share/TVC/sse/ampliseqexome_germline_p1_hiq_motifset.txt')
        if plan['samplePrepKitName'] == 'Ion AmpliSeq Exome Kit' and plan['sequencekitname'] == 'Ion S5 Sequencing Kit':
            options['error_motifs'] =  os.path.join(DIRNAME,'share/TVC/sse/ampliseqexome_germline_p1_hiq_motifset.txt')
        if plan['samplePrepKitName'] == 'Ion AmpliSeq Exome Kit' and expmeta['chiptype'] == '540' and plan['sequencekitname'] == 'IonProtonIHiQ':
            options['error_motifs'] =  os.path.join(DIRNAME,'share/TVC/sse/ampliseqexome_germline_p1_hiq_motifset.txt')
    except:
        pass

    with open(DIRNAME+'/pluginMedia/parameter_sets/parameter_sets.json','r') as f:
        built_in_parameters = json.load(f, parse_float=str)

    for reload_parameters in built_in_parameters:
        if configuration not in reload_parameters["meta"]["replaces"]:
        #if reload_parameters["meta"]["configuration"] != configuration:
            continue

        options['original_parameters'] = copy.deepcopy(startplugin_json['pluginconfig'])
        options['parameters'] = reload_parameters

        if 'meta' not in options['parameters']:
            options['parameters']['meta'] = {}
        if 'configuration' not in options['parameters']['meta']:
            options['parameters']['meta']['configuration'] = configuration

        options["original_config_line1"] = options['original_parameters']['meta'].get('name','Legacy '+configuration)
        options["original_config_line2"] = ''
        if options['original_parameters']['meta'].get('configuration',''):
            options["original_config_line2"] += options['original_parameters']['meta']['configuration'] + ', '
        options["original_config_line2"] += 'TS version: ' + options['original_parameters']['meta'].get('ts_version','5.0')
        break

    options["config_line1"] = options['parameters']['meta'].get('name','Legacy '+configuration)
    options["config_line2"] = ''
    if options['parameters']['meta'].get('configuration',''):
        options["config_line2"] += options['parameters']['meta']['configuration'] + ', '
    options["config_line2"] += 'TS version: ' + options['parameters']['meta'].get('ts_version','5.0')

    # Ensure nonstandard unicode characters are eliminated from config_line1, and original_config_line1

    options["config_line1"] = unicode_cleanup(options["config_line1"])
    if 'original_config_line1' in options:
        options["original_config_line1"] = unicode_cleanup(options["original_config_line1"])



    options['parameters']['meta']['tvcargs'] = startplugin_json['pluginconfig']['meta'].get('tvcargs','')
    if not options['parameters']['meta']['tvcargs']:
        options['parameters']['meta']['tvcargs'] = 'tvc'
    # Call tvc -v to get the version string
    tvc_args = options['parameters'].get('meta',{}).get('tvcargs','tvc')
    if tvc_args == 'tvc' and os.path.exists(DIRNAME + '/tvc'):   # try local binary first, then go to global one
        tvc_args = DIRNAME + '/tvc'
    options['tvc_version'] = execute_output(tvc_args + ' -v').splitlines()[0]
    if options['tvc_version'].endswith('- Torrent Variant Caller'):
        options['tvc_version'] = options['tvc_version'][:-24].strip()

    # These two values are needed to display the 'Output Directory' in the HTML pages
    options['plugin_name']               = startplugin_json['runinfo'].get('plugin_name','')
    options['pluginresult']              = startplugin_json['runinfo'].get('pluginresult','')

    options['run_name']                  = startplugin_json['expmeta'].get('run_name','Current run')
    options['has_barcodes']              = startplugin_json['expmeta'].get('barcodeId','') != ''

    options['trim_reads']   = startplugin_json['pluginconfig']['meta'].get('trimreads',True)
    options['barcode_mode'] = startplugin_json['pluginconfig']['meta'].get('barcode_mode','match')

    try:
        options['start_mode']               = 'Manual start'
        options['library_type']             = startplugin_json['pluginconfig']['meta']['librarytype']
        options['reference_genome_name']    = startplugin_json['pluginconfig']['meta']['reference']
#       options['targets_name']             = startplugin_json['pluginconfig']['meta']['targetregions_id']
        options['targets_bed_unmerged']     = startplugin_json['pluginconfig']['meta']['targetregions']
#       options['targets_bed_merged']       = startplugin_json['pluginconfig']['meta']['targetregions_merge']
#       options['hotspots_name']            = startplugin_json['pluginconfig']['meta']['targetloci_id']
        options['hotspots_bed_unmerged']    = startplugin_json['pluginconfig']['meta']['targetloci']
#       options['hotspots_bed_merged']      = startplugin_json['pluginconfig']['meta']['targetloci_merge']
    except:
        options['start_mode']               = 'Auto start'
        options['library_type']             = startplugin_json.get('plan',{}).get('runType',None)
        options['reference_genome_name']    = startplugin_json['runinfo'].get('library','')
        options['targets_bed_unmerged']     = startplugin_json.get('plan',{}).get('bedfile','')
        options['hotspots_bed_unmerged']    = startplugin_json.get('plan',{}).get('regionfile','')

    cleanup_options(options)

    return options


def cleanup_options(options):

    options['reference_genome_fasta']       = '/results/referenceLibrary/tmap-f3/' + options['reference_genome_name'] + '/' + options['reference_genome_name'] + '.fasta' #TODO

    if not options['targets_bed_unmerged'] or options['targets_bed_unmerged'] == "none":
        options['targets_bed_unmerged']     = ""
        options['targets_bed_merged']       = ""
        options['targets_name']             = ""
        options['trim_reads']               = False
    else:
        options['targets_bed_merged']       = options['targets_bed_unmerged'].replace('/unmerged/detail/','/merged/plain/')
        options['targets_name']             = os.path.basename(options['targets_bed_unmerged'])[:-4]

    if not options['hotspots_bed_unmerged'] or options['hotspots_bed_unmerged'] == "none":
        options['hotspots_bed_unmerged']    = ""
        options['hotspots_bed_merged']      = ""
        options['hotspots_name']            = ""
    else:
        options['hotspots_bed_merged']      = options['hotspots_bed_unmerged'].replace('/unmerged/detail/','/merged/plain/')
        options['hotspots_name']            = os.path.basename(options['hotspots_bed_unmerged'])[:-4]

    options['has_targets']                  = options['targets_name']
    options['has_hotspots']                 = options['hotspots_name']

    reference_genome_fasta_local = os.path.join(TSP_FILEPATH_PLUGIN_DIR, os.path.basename(options['reference_genome_fasta']))
    if not os.path.lexists(reference_genome_fasta_local):
        os.symlink(options['reference_genome_fasta'], reference_genome_fasta_local)

    reference_genome_fasta_index_local = os.path.join(TSP_FILEPATH_PLUGIN_DIR, os.path.basename(options['reference_genome_fasta']) + '.fai')
    if not os.path.lexists(reference_genome_fasta_index_local):
        os.symlink(options['reference_genome_fasta'] + '.fai', reference_genome_fasta_index_local)

    # Get local copy of BED files (may be deleted from system later)
    if options['has_targets']:
        if not os.path.exists( options['targets_bed_unmerged']):
            printtime('ERROR: Cannot locate target regions file: ' +  options['targets_bed_unmerged'])
            return 1
        if not os.path.exists(options['targets_bed_merged']):
            printtime('ERROR: Cannot locate merged target regions file: ' + options['targets_bed_merged'])
            return 1
        target_file = "%s/%s" % (TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['targets_bed_unmerged']))
        if not os.path.exists(target_file):
            shutil.copy(options['targets_bed_unmerged'], target_file)

            add_output_file('target_regions_bed', os.path.basename(options['targets_bed_unmerged']))

    # create 3 files in TSP_FILEPATH_PLUGIN_DIR and re-evaluate 'has_hotspots' and 'hotspots_vcf'
    if options['has_hotspots']:
        if not os.path.exists(options['hotspots_bed_unmerged']):
            printtime('ERROR: Cannot locate hotspots file: ' +  options['hotspots_bed_unmerged'])
            return 1
        if not os.path.exists(options['hotspots_bed_merged']):
            printtime('ERROR: Cannot locate merged hotspots file: ' + options['hotspots_bed_merged'])
            return 1

        options['hotspots_bed_unmerged_local']     = os.path.join(TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['hotspots_bed_unmerged']))
        options['hotspots_bed_unmerged_leftalign'] = os.path.join(TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['hotspots_bed_unmerged'][:-4] + '.left.bed'))
        options['hotspots_vcf']                    = os.path.join(TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['hotspots_bed_unmerged'][:-4] + '.hotspot.vcf'))

        if not os.path.exists(options['hotspots_bed_unmerged_local']):
            shutil.copy(options['hotspots_bed_unmerged'], options['hotspots_bed_unmerged_local'])

            prepare_hotspots_command  = 'tvcutils prepare_hotspots'
            prepare_hotspots_command += '  --input-bed %s' % options['hotspots_bed_unmerged']
            prepare_hotspots_command += '  --reference %s' % options['reference_genome_fasta']
            prepare_hotspots_command += '  --left-alignment on'
            prepare_hotspots_command += '  --allow-block-substitutions on'
            prepare_hotspots_command += '  --output-bed %s' % options['hotspots_bed_unmerged_leftalign']
            prepare_hotspots_command += '  --output-vcf %s' % options['hotspots_vcf']
            if options['has_targets']:
                prepare_hotspots_command += '  --unmerged-bed %s' % options['targets_bed_unmerged']
            run_command(prepare_hotspots_command, 'Generate filtered, left-aligned, and merged hotspot VCF file')

            hotspot_file_empty = True
            try:
                f = open(options['hotspots_vcf'], 'r')
                for line in f:
                    if not line or line.startswith('#'):
                        continue
                    hotspot_file_empty = False
                    break
            except:
                traceback.print_exc()
                pass

            if hotspot_file_empty:
                printtime('Filtered hotspot file has no hotspot entries. Disabling hotspots')
                options['has_hotspots'] = False
                options['hotspots_vcf'] = ""
            else:
                #run_command('bgzip -c %s/hotspot.vcf > %s/hotspot.vcf.gz' % (TSP_FILEPATH_PLUGIN_DIR,TSP_FILEPATH_PLUGIN_DIR), 'Generate compressed hotspot vcf')
                #run_command('tabix -p vcf %s/hotspot.vcf.gz' % (TSP_FILEPATH_PLUGIN_DIR), 'Generate index for compressed hotspot vcf')
                add_output_file('hotspots_bed', os.path.basename(options['hotspots_bed_unmerged']))

    if options['library_type'] in ["wholegenome",'WGNM','GENS']:
        options['library_type'] = "Whole Genome"
        options['trim_reads']   = False
    elif options['library_type'] in ["ampliseq",'AMPS','AMPS_EXOME','AMPS_DNA_RNA','AMPS_DNA']:
        options['library_type'] = "AmpliSeq"
    elif options['library_type'] in ["targetseq",'TARS']:
        options['library_type'] = "TargetSeq"
        options['trim_reads']   = False
    elif not options['library_type']:
        return {'error':'Automatic analysis was not performed. Cannot determine library type from plan.'}
    else:
        return {'error':'Automatic analysis was not performed. Library type "%s" is not supported.' % options['library_type']}


    if options['library_type'] == "AmpliSeq" and not options['has_targets']:
        return {'error':'Analysis aborted: AmpliSeq runs must have target regions specified.'}

    # These keys are not part of the parameter set
    if 'librarytype' in options['parameters']['meta']:
        del options['parameters']['meta']['librarytype']
    if 'targetregions_id' in options['parameters']['meta']:
        del options['parameters']['meta']['targetregions_id']
    if 'targetregions' in options['parameters']['meta']:
        del options['parameters']['meta']['targetregions']
    if 'targetregions_merge' in options['parameters']['meta']:
        del options['parameters']['meta']['targetregions_merge']
    if 'targetloci_id' in options['parameters']['meta']:
        del options['parameters']['meta']['targetloci_id']
    if 'targetloci' in options['parameters']['meta']:
        del options['parameters']['meta']['targetloci']
    if 'targetloci_merge' in options['parameters']['meta']:
        del options['parameters']['meta']['targetloci_merge']
    if 'user_selections' in options['parameters']['meta']:
        del options['parameters']['meta']['user_selections']





def get_bam_reference_short_name(bam):
    proc = subprocess.Popen(['samtools', 'view', '-H', bam], stdout=subprocess.PIPE)
    lines = proc.stdout.readlines()
    for line in lines:
        if line.startswith('@PG\tID:tmap'):
            v = line.split(' ')
            fasta = [i for i in v if i.endswith('.fasta')]
            assert len(fasta) == 1, '>1 fasta\n'
            fasta = fasta[0]
            break
    short_name = re.search(".*/(.*)\.fasta", fasta).group(1)
    return short_name
    
def tmap(cmd, new_aligned_bam):
    cmd += " | samtools sort -m 1000M -l1 -@12 - " + new_aligned_bam[:-4]
    printtime(cmd)
    subprocess.call(cmd,shell=True)
    #cmd = "samtools sort " + new_aligned_bam + " " + new_aligned_bam[:-4]
    #print(cmd)
    #subprocess.call(cmd,shell=True)
    cmd = "samtools index " + new_aligned_bam
    printtime(cmd)
    subprocess.call(cmd,shell=True)
    return new_aligned_bam

def runTmap(aligned_bam, new_reference, unaligned_bam, parameters, new_aligned_bam):
    returncode = 0
    files_string = " -n 24 -f " + new_reference + " -r " + unaligned_bam + " -v -Y -u --prefix-exclude 5 -o 2 "
    cmd = parameters.replace(" ... ", files_string)
    out = ""
    try:
        proc = subprocess.Popen(["samtools", "view", "-H", aligned_bam],stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        (out, err) = proc.communicate()
        returncode = proc.returncode
    except subprocess.CalledProcessError:
        returncode = 1
        error(dir + " variant_caller_pipeline run failed.")
    except OSError:
        returncode = 1
        error(dir + " variant_caller_pipeline run failed.")
    pos = out.find("ID:tmap")
    if pos == -1:
        return aligned_bam
    else:
        out = out[pos:]
        pos = out.find ("CL:")
        if pos == -1:
            return aligned_bam
        out = out[pos + 3:]
        pos = out.find("	VN:")
        if pos == -1:
            return aligned_bam
        out = out[:pos]
        pos = out.find("\n")
        if pos != -1:
            out = out[:pos]
        out = out.replace(" -i ", " -r ")
        
        q_param = ""
        posq = out.find(" -q ")
        if (posq != -1):
            posq += 4
            q_param = " -q "
            temp = out[posq:]
            while (out[posq] == " "):
                posq += 1
            while (out[posq] != " "):
                q_param += out[posq]
                posq += 1
        files_string = q_param + files_string
        cmd = parameters.replace(" ... ", files_string)

        posq = out.find(" -q ")
        pos1 = out.find(" -n ")
        pos2 = out.find(" -f ")
        pos3 = out.find(" -r ")
        pos4 = out.find(" -o ")
        if pos1 == -1 or pos2 == -1 or pos3 == -1 or pos4 == -1:
            return tmap(cmd, new_aligned_bam)
        if pos1 > pos2 or pos2 > pos3 or pos3 > pos4:
            return tmap(cmd, new_aligned_bam)
        pos = out.find(" -n ")
        if posq != -1 and posq < pos:
            pos = posq
        part_1 = out[:pos]
        pos = out.find(" -f")
        reference = out[pos+4:]
        pos = reference.find(" -r")
        reference = reference[:pos]
        pos = out.find(" -o 2 ")
        part_2 = out[pos+6:]
        oldcmd = "tmap " + part_1 + files_string + part_2
        cmd = "tmap " + part_1 + files_string + part_2
        if ((reference != new_reference) or (parameters != "")):
            if (parameters.find(" ... ") != -1): 
                cmd = parameters.replace(" ... ", files_string)
            elif (parameters != ""): 
                part_2 = parameters
                cmd = "tmap " + part_1 + files_string + part_2
            if ((reference != new_reference) or (cmd != oldcmd)):
                return tmap(cmd, new_aligned_bam)
    return aligned_bam
    
def combine_files(combinedfilename, myfile):
    try:
        file_in = open(myfile, "r")
        if (os.path.isfile(combinedfilename)):
            file_out = open(combinedfilename, "a")
            for line in file_in:
                if (not line.startswith("#")): file_out.write(line)
            file_out.close()
        else:
            file_out = open(combinedfilename, "w")
            for line in file_in:
                file_out.write(line)
            file_out.close()
        file_in.close()
    except:
        pass

def print_options(vc_options, parameters_file):
    global TSP_FILEPATH_PLUGIN_DIR
    global startplugin_json
    
    printtime('Variant Caller plugin run options:')
    printtime('  Plugin name                : ' + startplugin_json['runinfo'].get('plugin_name',''))
    printtime('  Plugin start mode          : ' + vc_options['start_mode'])
    printtime('  Variant Caller version     : ' + vc_options['tvc_version'])
    printtime('  Run is barcoded            : ' + str(vc_options['has_barcodes']))
    printtime('  Genome                     : ' + vc_options['reference_genome_name'])
    printtime('  Library Type               : ' + vc_options['library_type'])
    printtime('  Target Regions             : ' + (vc_options['targets_name'] if vc_options['has_targets'] else 'Not using'))
    printtime('  Hotspots                   : ' + (vc_options['hotspots_name'] if vc_options['has_hotspots'] else 'Not using'))
    if 'original_parameters' in vc_options:
        printtime('  Requested Parameters       : ' + vc_options["original_config_line1"])
        printtime('                               ' + vc_options["original_config_line2"])
        printtime('  Auto-Updated Parameters    : ' + vc_options["config_line1"])
        printtime('                               ' + vc_options["config_line2"])
    else:
        printtime('  Used Parameters            : ' + vc_options['config_line1'])
        printtime('                               ' + vc_options["config_line2"])

    printtime('  Trim Reads                 : ' + str(vc_options['trim_reads']))

    printtime('')
    printtime('Used files:')
    printtime('  Reference Genome           : ' + vc_options['reference_genome_fasta'])
    printtime('  Parameters file            : ' + os.path.join(TSP_FILEPATH_PLUGIN_DIR,parameters_file))
    if 'parameters_source' in vc_options:
        printtime('  Parameters source file     : ' + vc_options['parameters_source'])

    if vc_options['has_targets']:
        printtime('  Target unmerged BED        : ' + vc_options['targets_bed_unmerged'])
        printtime('  Target merged BED          : ' + vc_options['targets_bed_merged'])
    if vc_options['has_hotspots']:
        printtime('  Hotspots unmerged BED      : ' + vc_options['hotspots_bed_unmerged'])
        printtime('  Hotspots merged BED        : ' + vc_options['hotspots_bed_merged'])
    printtime('  Barcode mode               : ' + vc_options['barcode_mode'])
    printtime('')
        
def plugin_main():

    global PLUGIN_DEV_SKIP_VARIANT_CALLING
    global DIRNAME
    global TSP_URLPATH_PLUGIN_DIR
    global TSP_FILEPATH_PLUGIN_DIR
    global startplugin_json
    global output_files

    parser = OptionParser()
    parser.add_option('-d', '--install-dir', help='Directory containing plugin files', dest='install_dir')
    parser.add_option('-o', '--output-dir', help='Directory for results files', dest='output_dir')
    parser.add_option('-u', '--output-url', help='URL matching the output directory', dest='output_url')
    parser.add_option('-r', '--report-dir', help='Directory containing analysis report files', dest='report_dir')
    parser.add_option('-s', '--skip-tvc', help='(debug) Skip variant calling and reuse existing results', dest='skip_tvc', action="store_true", default=False)
    (options, args) = parser.parse_args()


    DIRNAME                     = options.install_dir    #os.environ['DIRNAME']         # home directory for the plugin files
    TSP_FILEPATH_PLUGIN_DIR     = options.output_dir     #os.environ['TSP_FILEPATH_PLUGIN_DIR'] # target plugin results directory
    ANALYSIS_DIR                = options.report_dir     #os.environ['ANALYSIS_DIR'] # main report directory
    TSP_URLPATH_PLUGIN_DIR      = options.output_url
    PLUGIN_DEV_SKIP_VARIANT_CALLING = options.skip_tvc

    settings.configure(DEBUG=True, TEMPLATE_DEBUG=True, TEMPLATE_DIRS=((DIRNAME+'/templates'),))

    subprocess.call('rm -f %s/results.json' % TSP_FILEPATH_PLUGIN_DIR,shell=True)

    printtime('')
    printtime('Variant Caller Plugin started')
    printtime('')

    try:
        json_file = open(os.path.join(TSP_FILEPATH_PLUGIN_DIR,'startplugin.json'), 'r')
        startplugin_json = json.load(json_file,parse_float=str)
        json_file.close()
    except:
        printtime('ERROR: Failed to load and parse startplugin.json')
        return 1


    # Uncomment to emulate autorun:
    #startplugin_json['pluginconfig'] = {}

    vc_options = get_options(startplugin_json)

    if 'error' in vc_options:
        printtime(vc_options['error'])
        generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS),vc_options['error'], vc_options)
        return 1

    f = open(os.path.join(TSP_FILEPATH_PLUGIN_DIR,BASENAME_PARAMETERS_JSON),'w')
    json.dump(vc_options['parameters'],f,indent=4)
    f.close()

    add_output_file('parameters_json', BASENAME_PARAMETERS_JSON)

    # Parameters from plugin customization
    if (not 'barcodes' in startplugin_json['pluginconfig']):
        print_options(vc_options, BASENAME_PARAMETERS_JSON)

    PLUGIN_HS_ALIGN_DIR = TSP_FILEPATH_PLUGIN_DIR + '/hs_align'

    # Remove previous results to avoid displaying old before ready
    subprocess.call('rm -f %s/%s' % (TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS),shell=True)
    subprocess.call('rm -f %s' % (TSP_FILEPATH_PLUGIN_DIR + '/results.json'),shell=True)
    subprocess.call('rm -f %s/*.bed' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/*.fasta' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/*.fasta.fai' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -rf %s/*.bam*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -rf %s' % (PLUGIN_HS_ALIGN_DIR),shell=True)
    #subprocess.call('rm -f %s/hotspot*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/variant*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/allele*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/*.xls' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/*.log' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -f %s/*.done' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('rm -rf %s/lifegrid' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)

    if not PLUGIN_DEV_SKIP_VARIANT_CALLING:
        subprocess.call('rm -f %s/SNP*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
        subprocess.call('rm -f %s/indel*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)
        subprocess.call('rm -f %s/TSVC*' % (TSP_FILEPATH_PLUGIN_DIR),shell=True)


    printtime('Results folder initialized')

    # Make links to js/css used for barcodes table and empty results page
    subprocess.call('ln -sf "%s/js" "%s"' % (DIRNAME,TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('ln -sf "%s/css" "%s"' % (DIRNAME,TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('ln -sf %s/scripts/*.php3 "%s"' % (DIRNAME,TSP_FILEPATH_PLUGIN_DIR),shell=True)

    #TODO, some values in here are barcode specific
    results_json = {
        'Aligned Reads'     : vc_options['run_name'],  # TODO,BUG?
        'Library Type'      : vc_options['library_type'],
        'Configuration'     : vc_options['parameters']['meta']['configuration'],
        'Target Regions'    : (vc_options['targets_name'] if vc_options['has_targets'] else 'Not using'),
        'Target Loci'       : (vc_options['hotspots_name'] if vc_options['has_hotspots'] else 'Not using'),
        'Trim Reads'        : vc_options['trim_reads'],
        'files'             : []
    }
    if vc_options['has_barcodes']:
        results_json['barcoded'] = 'true'
        results_json['barcodes'] = {}
    else:
        results_json['barcoded'] = 'false'


    if vc_options['has_barcodes']:
        barcode_sample_info = {}
        samples = startplugin_json.get('plan',{}).get('barcodedSamples',"")
        if samples and not isinstance(samples,dict):
            samples = json.loads(samples)
        
        for k,v in samples.iteritems():
            barcode_sample_info.update(v.get('barcodeSampleInfo',{}))

        printtime("info: %s" % barcode_sample_info) # TODO, DEBUG

        datasets = []

        # Load barcode list
        if os.path.exists(ANALYSIS_DIR + '/barcodeList.txt'):
            with open(ANALYSIS_DIR + '/barcodeList.txt','r') as bc_list_file:
                for line in bc_list_file:
                    if not line.startswith('barcode '):
                        continue
                    dataset = {}
                    dataset['is_barcode'] = True
                    dataset['name'] = line.split(',')[1]
                    dataset['bam'] = os.path.join(ANALYSIS_DIR, dataset['name'] + '_rawlib.bam')
                    datasets.append(dataset)

        '''
        # Non-barcoded sample
        dataset = {}
        dataset['is_barcode'] = False
        dataset['name'] = 'no_barcode'
        dataset['bam'] = os.path.join(ANALYSIS_DIR, 'rawlib.bam')
        datasets.append(dataset)
        '''


        startplugin_json_save = copy.deepcopy(startplugin_json)
        datasets_data = []
        for dataset in datasets:

            printtime("name: %s" % dataset['name']) # TODO, DEBUG

            dataset['parameters'] = vc_options['parameters']

            override = False
            configuration_name = 'default'
            tmap_args = startplugin_json['pluginconfig'].get('meta',{}).get('tmapargs','')
            startplugin_json = copy.deepcopy(startplugin_json_save)
            if ('barcodes' in startplugin_json['pluginconfig']):
                for barcode_params in startplugin_json['pluginconfig']['barcodes']:
                    if (barcode_params['bam'] == os.path.basename(dataset['bam'])):
                        override = True
                        startplugin_json = copy.deepcopy(barcode_params['json'])
                        startplugin_json['plan'] = startplugin_json_save['plan']
                        startplugin_json['expmeta'] = startplugin_json_save['expmeta']
                        startplugin_json['runinfo'] = startplugin_json_save['runinfo']
                        # Uncomment to emulate autorun:
                        configuration_name = startplugin_json['pluginconfig'].get('meta',{}).get('configuration_name','')
                        tmap_args = startplugin_json['pluginconfig'].get('meta',{}).get('tmapargs','')
                        vc_options = get_options(startplugin_json)
                        dataset['parameters'] = vc_options['parameters']

                        if 'error' in vc_options:
                            printtime(vc_options['error'])
                            generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS),vc_options['error'], vc_options)
                            return 1

                        f = open(os.path.join(TSP_FILEPATH_PLUGIN_DIR,BASENAME_PARAMETERS_JSON),'w')
                        json.dump(vc_options['parameters'],f,indent=4)
                        f.close()
                        
                        f = open(os.path.join(TSP_FILEPATH_PLUGIN_DIR,dataset['name'] + "_parameters.json"),'w')
                        json.dump(vc_options['parameters'],f,indent=4)
                        f.close()
                        add_output_file('parameters_json', dataset['name'] + "_parameters.json")
                        
                        print_options(vc_options, dataset['name'] + "_parameters.json")
                        
                        break;
                if (not override): continue
            if ('override' in startplugin_json['pluginconfig'].get('meta',{})): override = True

            # TODO: why is a barcode specific entry required to display the output directory?
            # These two values are needed to display the 'Output Directory' in the HTML pages
            dataset['plugin_name']               = vc_options['plugin_name']
            dataset['pluginresult']              = vc_options['pluginresult']
            # TODO: why is a barcode specific entry required to display global configuration settings in HTML pages?
            dataset['config_line1']              = vc_options['config_line1']
            dataset['config_line2']              = vc_options['config_line2']
            dataset['tvc_version']               = vc_options['tvc_version']

            dataset['run_name']                  = vc_options['run_name']
            dataset['library_type']              = vc_options['library_type']
            dataset['trim_reads']                = vc_options['trim_reads']
            dataset['has_error_motifs']          = vc_options['has_error_motifs']
            dataset['error_motifs']              = vc_options['error_motifs']
            if not PLUGIN_DEV_SKIP_VARIANT_CALLING:
                subprocess.call('rm -rf %s/%s' % (TSP_FILEPATH_PLUGIN_DIR,dataset['name']),shell=True)

            dataset['status'] = 'queued'

            if not os.path.exists(dataset['bam']):
                dataset['status'] = 'no bam file'
                continue

            if ((not override) and (dataset['name'] in barcode_sample_info)):

                dataset['reference_genome_name'] = barcode_sample_info[dataset['name']].get('reference','')
                dataset['targets_bed_unmerged']  = barcode_sample_info[dataset['name']].get('targetRegionBedFile','')
                dataset['hotspots_bed_unmerged'] = barcode_sample_info[dataset['name']].get('hotSpotRegionBedFile','')

                # assume 'dna' and 'DNA' and 'dNa' etc. are the same, assume DNA if no nucleotideType exists
                dataset['nuc_type']              = barcode_sample_info[dataset['name']].get('nucleotideType','DNA').upper()
            #elif not dataset['is_barcode']:
            #    dataset['reference_genome_name'] = vc_options['reference_genome_name']
            #    dataset['targets_bed_unmerged']  = vc_options['targets_bed_unmerged']
            #    dataset['hotspots_bed_unmerged'] = vc_options['hotspots_bed_unmerged']
            #    dataset['nuc_type']              = 'DNA'
            else:
                dataset['reference_genome_name'] = vc_options['reference_genome_name']
                dataset['targets_bed_unmerged']  = vc_options['targets_bed_unmerged']
                dataset['hotspots_bed_unmerged'] = vc_options['hotspots_bed_unmerged']
                dataset['nuc_type']              = 'DNA'

                if (not dataset['name'] in barcode_sample_info): printtime('Detected barcode ' + dataset['name'] + ' : Missing barcode sample info ')

            cleanup_options(dataset)
                
            # revert decision to support variant calling for RNA type
            if (dataset['nuc_type'] == ''): dataset['nuc_type'] = 'DNA'
            if dataset['nuc_type'] != 'DNA':
                printtime('Skipping barcode ' + dataset['name'] + ' : Unsupported nuc type ' + dataset['nuc_type'])
                dataset['status'] = 'Unsupported nuc type'
                continue
                
            if (dataset['reference_genome_name'] == ''):
                dataset['status'] = 'no reference genome'
                continue
            if not os.path.exists(dataset['reference_genome_fasta']):
                dataset['status'] = 'no reference genome'
                continue

            true_reference = get_bam_reference_short_name(dataset['bam'])
            if true_reference != dataset['reference_genome_name'] or tmap_args != "":
                if true_reference != dataset['reference_genome_name']:
                    printtime('Detected barcode ' + dataset['name'] + ' : Bam reference ' + true_reference +
                          ' different from run reference ' + dataset['reference_genome_name'])
                # TS-11423 Use previously aligned bam as unaligned bam. Previous alignment will be stripped off by tmap
                unaligned_bam = dataset['bam']
                new_aligned_bam = os.path.join(TSP_FILEPATH_PLUGIN_DIR, dataset['name'] + '_rawlib.' + 'realigned' + '.bam')
                dataset['bam'] = runTmap(dataset['bam'], vc_options['reference_genome_fasta'], unaligned_bam, tmap_args, new_aligned_bam)
                #continue

            if dataset['has_targets'] and dataset['targets_bed_unmerged'] != dataset['targets_bed_unmerged']:
                printtime('Detected barcode ' + dataset['name'] + ' : Barcode target region ' + dataset['targets_bed_unmerged'] +
                          ' different from run target region ' + dataset['targets_bed_unmerged'])
                #continue
            if not dataset['has_targets'] and dataset['targets_bed_unmerged']:
                printtime('Detected barcode ' + dataset['name'] + ' : Barcode has target region ' + dataset['targets_bed_unmerged'] +
                          ' when there is no run target region')
                #continue

            # Size enough to process? TODO - just get from datasets_basecaller.json
            if os.stat(dataset['bam']).st_size < BCFILE_MIN_SIZE:
                dataset['status'] = 'insufficient_reads'

            print("status: %s" % dataset['status'])

            datasets_data.append(dataset)

        printtime('')
        printtime('Processing %d datasets...' % len(datasets_data))

        all_datasets_successful = True

        for dataset in datasets_data:
            if dataset['status'] != 'queued':
                printtime('Skipping barcode ' + dataset['name'])
                continue

            dataset['status'] = 'in_progress'
            # TODO: why datasets_data ?
            generate_barcode_links_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS), datasets_data, dataset)

            # perform coverage anaysis and write content # TODO, wrong comment?
            printtime('')
            printtime('Processing dataset ' + dataset['name'])

            try:
                summary = call_variants(dataset)

                results_json['barcodes'][dataset['name']] = {}

                if dataset['has_targets']:
                    results_json['barcodes'][dataset['name']]['targets_bed']  = dataset['targets_bed_unmerged']
                if dataset['has_hotspots']:
                    results_json['barcodes'][dataset['name']]['hotspots_bed'] = dataset['hotspots_bed_unmerged']
                results_json['barcodes'][dataset['name']]['variants'] = summary.get('variants_total',{})
                results_json['barcodes'][dataset['name']]['hotspots'] = summary.get('hotspots_total',{})

                dataset['summary'] = summary
                dataset['status'] = 'completed'

            except:
                traceback.print_exc()
                all_datasets_successful = False
                dataset['status'] = 'error'

        printtime(' ')
        printtime('Task    : ' + 'Store per-barcode vcf files in a single zip file')
        zipfilename = '%s/%s.vcf.zip' % (TSP_FILEPATH_PLUGIN_DIR,vc_options['run_name'])
        for myfile in [('%s/%s/TSVC_variants_%s.vcf.gz' % (TSP_FILEPATH_PLUGIN_DIR,dataset['name'],dataset['name'])) for dataset in datasets_data if dataset['status'] == 'completed']:
            compress.make_zip(zipfilename, myfile, arcname=os.path.basename(myfile), use_sys_zip = False)
        for myfile in [('%s/%s/TSVC_variants_%s.vcf.gz.tbi' % (TSP_FILEPATH_PLUGIN_DIR,dataset['name'],dataset['name'])) for dataset in datasets_data if dataset['status'] == 'completed']:
            compress.make_zip(zipfilename, myfile, arcname=os.path.basename(myfile), use_sys_zip = False)
        printtime(' ')
        printtime(' ')
        printtime('Task    : ' + 'Store per-barcode xls files in a single zip file')
        zipfilename = '%s/%s.xls.zip' % (TSP_FILEPATH_PLUGIN_DIR,vc_options['run_name'])
        for myfile in [('%s/%s/alleles_%s.xls' % (TSP_FILEPATH_PLUGIN_DIR,dataset['name'],dataset['name'])) for dataset in datasets_data if dataset['status'] == 'completed']:
            compress.make_zip(zipfilename, myfile, arcname=os.path.basename(myfile), use_sys_zip = False)
        printtime(' ')
        printtime('Task    : ' + 'Store per-barcode cov files in a single zip file')
        combinedfilename = '%s/%s.cov.xls' % (TSP_FILEPATH_PLUGIN_DIR,vc_options['run_name'])
        for myfile in [('%s/%s/variant_allele_counts_%s.xls' % (TSP_FILEPATH_PLUGIN_DIR,dataset['name'],dataset['name'])) for dataset in datasets_data if dataset['status'] == 'completed']:
            combine_files(combinedfilename, myfile)
        printtime(' ')

        # TODO, dont pass datasets_data and dataset at the same times
        generate_barcode_links_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS), datasets_data, dataset)
        generate_barcode_links_block(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_BLOCK), datasets_data, dataset)

        if not all_datasets_successful:
            return 1



    else:   # Non-barcoded run

        generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS),
                                        'Variant calling still in progress', vc_options, autorefresh=True)
        vc_options['is_barcode'] = False
        vc_options['bam'] = os.path.join(ANALYSIS_DIR, 'rawlib.bam')

        if os.stat(vc_options['bam']).st_size < BCFILE_MIN_SIZE:
            generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS),
                                            'Bam file size too small.', vc_options)
            generate_nonbarcoded_block_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR, HTML_BLOCK),
                                            'Non-barcoded run: Bam file size is too small. Variant calling was not carried out.')
            return 0
        
        true_reference = get_bam_reference_short_name(vc_options['bam'])
        tmap_args = startplugin_json['pluginconfig'].get('meta',{}).get('tmapargs','')
        if true_reference != vc_options['reference_genome_name'] or tmap_args != "":
            if true_reference != vc_options['reference_genome_name']:
                printtime('Bam reference ' + true_reference +
                      ' different from run reference ' + vc_options['reference_genome_name'])
            # TS-11423 Use previously aligned bam as unaligned bam. Previous alignment will be stripped off by tmap
            unaligned_bam = vc_options['bam']
            new_aligned_bam = os.path.join(TSP_FILEPATH_PLUGIN_DIR, 'rawlib.' + 'realigned' + '.bam')
            vc_options['bam'] = runTmap(vc_options['bam'], vc_options['reference_genome_fasta'], unaligned_bam, tmap_args, new_aligned_bam)
                
        #if true_reference != vc_options['reference_genome_name']:
        #    generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS),
        #                                    'Run reference does not match bam reference.', vc_options)
        #    generate_nonbarcoded_block_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR, HTML_BLOCK),
        #                                    'Non-barcoded run: Run reference does not match bam reference. Variant calling was not carried out.')
        #    return 0

        try:
            summary = call_variants(vc_options)
            results_json['variants'] = summary.get('variants_total',{})
            results_json['hotspots'] = summary.get('hotspots_total',{})
        except:
            traceback.print_exc()
            generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS), 'An error occurred - check Log File for details', vc_options)
            return 1

    results_json['files'] = output_files
    out = open(TSP_FILEPATH_PLUGIN_DIR + '/results.json','w')
    json.dump(results_json,out,indent=4)
    out.close()

    printtime('')
    printtime('Variant Caller Plugin complete')
    printtime('')

    return 0


if __name__ == "__main__":

    exit(plugin_main())


